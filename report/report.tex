\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{float} % for H placement option
\usepackage{listings}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Hard disk failure data analysis\\}

\author{\IEEEauthorblockN{Luca Falasca}
\IEEEauthorblockA{\textit{0334722} \\
luca.falasca@students.uniroma2.eu
}
\and
\IEEEauthorblockN{Matteo Conti}
\IEEEauthorblockA{\textit{0323728} \\
matteo.conti97@students.uniroma2.eu
}\\
}


\maketitle

\begin{abstract}
\end{abstract}

\section{Introduzione}
\subsection{Descrizione del problema}
Il problema da affrontare consiste nell'eseguire il batch processing di dati di grandi dimensioni mediante l'uso di una pipeline basata su framework Big Data. Nello specifico, l'obiettivo è analizzare un dataset contenente informazioni relative ai fallimenti dei dischi rigidi per l'esecuzione di 3 query di analisi dei dati e delle prestazioni di esse.

Il dataset fornito è una versione ridotta di quello presentato nel Grand Challenge della conferenza ACM DEBS 2024. Delle numerose colonne presenti nel dataset, ne sono state selezionate cinque per l'esecuzione delle query, in particolare:
\begin{itemize}
    \item \textbf{date}: data della misurazione
    \item \textbf{serial\_number}: identificativo del disco rigido
    \item \textbf{failure}: indica se il disco rigido ha avuto una failure o meno
    \item \textbf{model}: modello del disco rigido
    \item \textbf{vault\_id}: identificativo del gruppo di storage server
    \item \textbf{power\_on\_hours}: ore di accensione del disco rigido
\end{itemize}
\subsection{Obiettivi}
\section{Pipeline}
Per la gestione dei dati relativi ai guasti dei dischi rigidi, è stata implementata una pipeline ETL il cui deploy è stato fatto tramite docker compose. La pipeline è composta da diverse componenti, ognuna delle quali svolge un compito specifico, in particolare sono stati utilizzati Apache NiFi per la data ingestion, HDFS per il data storage, Spark per il data processing e MongoDB per l'analytical data storage, di seguito verrano illustrati i dettagli di ciascuna componente.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{./res/pipeline_2.png}
    \caption{Pipeline}
    \label{fig:pipeline}
\end{figure} 
\subsection{Data Ingestion}
Per la data ingestion è stato utilizzato il framework Apache NiFi, il quale permette di creare dei flussi di dati a partire da diversi tipi di sorgente, definire delle operazioni di trasformazione sui flussi di dati e andare a memorizzare il risultato delle trasformazioni su datastore di diverso tipo. Nel nostro caso NiFi svolge le seguenti operazioni:
\begin{enumerate}
    \item Riceve tramite HTTP il file CSV contenente i dati relativi ai guasti dei dischi rigidi
    \item Effettua una query SQL (Fig. \ref{fig:nifi_query}) per selezionare solamente le cinque colonne di interesse del CSV ed effettuare pulizia dei dati rimuovendo righe contenenti valori nulli o invalidi. Il campo "s9\_power\_on\_hours" contiene dei valori 0, tuttavia essi non sono stati filtrati in quanto si assume che essi essi facciano riferimento a dischi che non hanno raggiunto l'ora di accensione al momento della misurazione.
    \item Effettua l'assegnazione esplicita dello schema ai dati, in quanto non facendolo NiFi non riconosce correttamente alcuni tipi di dato, interpretandoli come delle strutture
    \item Scrive i dati su HDFS in formato Parquet e CSV
\end{enumerate}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{./res/query_nifi.png}
    \caption{Query SQL filtraggio dati NiFi}
    \label{fig:nifi_query}
\end{figure} 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{./res/nifi_flow.png}
    \caption{Nifi flow}
    \label{fig:nifi_flow}
\end{figure} 
\subsection{Data Storage}
Come data lake in cui memorizzare i dati a a seguito del preprocessamento effettuato da parte di NiFi è stato utilizzato HDFS, il quale permette di memorizzare grandi quantità di dati su un cluster di nodi. Nel nostro caso il cluster è composto da:
\begin{itemize}
    \item 1 Namenode
    \item 2 Datanode
\end{itemize}
Non è stata utilizzata una struttura particolare per l'organizzazione dei file all'interno di HDFS, i file sono stati memorizzati tutti nella root directory.
\subsection{Data Processing}
Per il processamento dei dati e quindi l'esecuzione delle query è stato utilizzato il framework Spark, in particolare utilizzando la versione per python PySpark.Per queste query è stato utilizzato un cluster composto da uno spark master e quattro spark worker. Per farlo abbiamo utilizzato l'immagine bitami/spark nella versione 3.5.1. Per l'orchestrazione del cluster abbiamo utilizzato docker compose.
Il dataset filtrato è stato caricato dall'hdfs e i risultati delle query sono stati salvati in MongoDB utilizzando il connettore spark-mongodb.
Le query sono state eseguite sia a partire dal file in formato CSV che dal file in formato Parquet per valutare le differenze prestazionali tra un formato basato su colonne e uno basato su righe.
Per effettuare le query sono state utilizzate le API per i Dataframe e non quelli per gli RDD. 
Inoltre siccome spesso alcune query sono composte sul finale di operazioni di trasformazioni, ma esse vengono finalizzate solo in una operazione di azione, in molti casi l'azione finale sarebbe stata quella della scrittura del risultato sul DB. Dato che questa operazione dipende dalla grandezza del risultato della query, abbiamo deciso per unificare la metodologia di acquisizione delle misure di prestazioni di inserire un'azione di show() che ha lo scopo di forzare l'esecuzione della query e quindi di misurare il tempo di esecuzione.
Le query sono state implementate 
\subsubsection{Query 1}
La query 1 prevede di calcolare per ogni giorno, per ogni vault (facendo riferimento al campo vault id), il numero totale di fallimenti, in particolare occorre determinare la lista di vault che hanno subito esattamente 4, 3 e 2 fallimenti.

Per prima cosa è stato fatto il drop delle colonne inutili per la query, che in questo caso sono "serial\_number", "model" e "s9\_power\_on\_hours", successivamente è stato fatto il group by per "date" e "vault\_id" e la somma dei fallimenti (essendo i fallimenti rappresentati dai valori 0 e 1 la loro somma rappresenta il numero di fallimenti). Infine è stato effettuato un filtraggio che seleziona solo i vault che hanno subito un numero di fallimenti $\geq2$ e $\leq4$ 

\subsubsection{Query 2}
La query 2 è composta di due parti:
\begin{enumerate}
    \item Calcolare la classifica dei 10 modelli di hard disk che hanno subito il maggior numero di fallimenti, riportando il modello di hard disk e il numero totale di fallimenti subiti dagli hard disk di quello specifico modello
    \item Calcolare la classifica dei 10 vault che hanno registrato il maggior numero di
    fallimenti riportando, per ogni vault, il numero di fallimenti e la lista (senza ripetizioni) dei modelli di
    hark disk soggetti ad almeno un fallimento
\end{enumerate}
Anche in questo caso la prima cosa viene fatto il drop delle colonne inutili per la query, che in questo caso sono "serial\_number", "s9\_power\_on\_hours" e "date", successivamente viene fatto un cache() per mantenere in memoria il Dataframe ed evitare di doverlo costruire più volte, in quanto esso verrà utilizzato per entrambe le parti della query.
\subsubsection{Query 3}
La query 3 prevede di calcolare il minimo, 25-esimo, 50-esimo, 75-esimo percentile e massimo delle ore di funzionamento (campo s9\_power\_on\_hours) degli hark disk che hanno subito fallimenti e degli hard disk che non hanno subito fallimenti, indicando anche il numero totale di eveneti utilizzati per il calcolo delle statistiche.
Dato che il campo "s9\_power\_on\_hours" è un campo cumulativo, per ottenere il tempo di funzionamento è necessario guardare la data più recente disponibile.
Anche in questo caso per prima cosa è stato fatto il drop delle colonne non necessarie che in questo caso sono "model" e "vault\_id", successivamente è stato fatto un cache() per mantenere in memoria il Dataframe ed evitare di doverlo costruire più volte, in quanto esso verrà utilizzato per due operazioni diverse. A questo punto dal dataframe ne vengono creati altri due, una che contenenete solamente dischi che hanno subito fallimenti e una che contenente solamente dischi che non hanno subito fallimenti. Per entrambi i Dataframe sono state fatte le seguenti cose: un group by per "serial\_number" e il calcolo del massimo delle ore di funzionamento attraverso la funzione first() sulla colonna della data. La funzione first() ottiene il primo valore della colonna, ciò è stato possibile grazie al fatto che il dataset è ordinato per data. È quindi stato preferito questo approccio rispetto prendere il max per questioni di performance. Poi prima di fare il calcolo dei quantili è stato effettuato il drop delle colonne "serial\_number" e "dates" in modo da ottenere solo il dato delle ore di funzionamento per le failure 0 e 1 e allegerire le successive elaborazioni. A questo punto è stato effettuato il calcolo dei quantili utilizzando la funzione approxQuantile che permette di calcolare i quantili approssimati in modo efficiente e anche il minimo e massimo che corrispondono al quanitle 0 e 100. Infine è stato creato un nuovo dataframe che contiene i risultati delle statistiche dei quantili unito alla count delle righe del dataframe. Come detto in precedenza le operazioni fino a questo punto sono state fatte in egual modo per il dataframe con i fallimenti e per quello senza, a questo punto è stato fatto un union dei due dataframe per ottenere il risultato finale.  

\subsection{Analytical Data Storage}
I risultati delle query sono stati memorizzati in un database a documento, in particolare MongoDB (Fig. \ref{fig:mongo}).
Per organizzare i dati, è stata creata una collezione per ciascuna query, in cui ogni documento rappresenta una riga dei risultati ottenuti dalla query stessa. Inoltre, è stata creata un'altra collezione che contiene le performance, in particolare vengono riportati il timestamp dell'esperimento, la query, il formato del dataset su cui la query è stata eseguita ed il tempo di esecuzione della query stessa.
\begin{figure}[H]
    \centerline{\includegraphics[width=0.5\textwidth]{res/mongo.png}}
    \caption{Schema del database MongoDB.}
    \label{fig:mongo}
\end{figure}
\subsection{Analisi delle prestazioni}
\vspace{12pt}
\end{document}

    